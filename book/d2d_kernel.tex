% !TEX root = xelatex_main.tex
\chapter{理解内存合并访问}

有很多内核里浮点或整形运算量不能完全掩盖内存访问延迟，也就是说它们的性能是被内存的带宽限制的（memory bounded)。 对于这些内核来说，理解并尽可能地满足CUDA里的内存合并访问（Coalesced Memory Access）是提升内核性能的最主要手段之一。 让我们从一个简单的例子来看看到底怎么样才能最大限度地利用GPU上的巨大带宽。这个例子的任务是用内核函数来实现两块GPU内存之间的拷贝。从功能上来说这是CUDA提供的cudaMemcpy的一种（cudaMemcpyDeviceToDevice)，貌似没有什么必要，但是我们可以和这个API做一下性能上的比较，看看我们的实现是否和CUDA提供的cudaMemcpy相仿。同时也能理解一下保证合并访问的条件。

作为格调更高的C++程序员，我们当然情不自禁地从模版开始码字了：template <typename T> ...，第一版本如下：
\myvspace
\lstinputlisting[language=C++]{\JushaBase/tests/src/d2d_direct.cu}
代码很简单，无非就是一个循环（第3，4行）把输入拷到输出。内核调用的时候用了一下jusha里提供的缺省的blockDim和gridDim（第9，10行）。
如果直接用CUDA提供的cudaMemcpy的话，就一句，更简单：
\myvspace
\lstinputlisting[language=C++]{\JushaBase/tests/src/cudamemcpy.cu}

首先我们来比较一下性能，这里画出不同数据大小在各种数据类型的测量带宽曲线，类型有char, int/float, double, float3, float4。

可以看到cudaMemcpy性能非常稳定，不管是任何类型，在  处都能达到理论带宽的％。而我们写的函数则优劣不齐。好的（float4, int, double）能接近cudaMemcpy，差的只能达到二分之一(char) 或者（flaot3）左右。

现在回到CUDA文档里合并访问的要求，如下：
\begin{itemize}
\item 对于每16个线程（半个warp）来说，
\end{itemize}

费了很大的周折，我们终于实现了一个无论数据类型，无论地址偏移的高性能的拷贝，归根结底还是要回到用C语言的实现。